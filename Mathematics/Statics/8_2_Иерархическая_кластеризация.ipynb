{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/Statics/8_2_%D0%98%D0%B5%D1%80%D0%B0%D1%80%D1%85%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Иерархическая кластеризация\n",
        "\n",
        "### Введение\n",
        "\n",
        "Иерархическая кластеризация — это метод анализа данных, который организует объекты в группы (кластеры) и представляет эти группы в виде иерархического дерева, называемого дендрограммой. Этот метод широко применяется в различных областях, таких как биология, маркетинг, социология и других.\n",
        "\n",
        "### 1. Основные понятия\n",
        "\n",
        "#### Кластеризация\n",
        "\n",
        "Кластеризация — это процесс группировки множества объектов в подмножества (кластеры), так что объекты в одном кластере более похожи друг на друга, чем на объекты из других кластеров.\n",
        "\n",
        "#### Иерархическая кластеризация\n",
        "\n",
        "Иерархическая кластеризация делится на две основные категории:\n",
        "\n",
        "1. **Агломеративная (снизу вверх)**: начинается с каждого объекта как отдельного кластера и последовательно объединяет их в более крупные кластеры.\n",
        "2. **Делительная (сверху вниз)**: начинается с одного кластера, содержащего все объекты, и последовательно разделяет его на более мелкие кластеры.\n",
        "\n",
        "### 2. Алгоритм агломеративной кластеризации\n",
        "\n",
        "Агломеративная кластеризация проходит несколько этапов:\n",
        "\n",
        "1. **Инициализация**: Каждый объект рассматривается как отдельный кластер.\n",
        "2. **Вычисление расстояний**: Для каждого кластера вычисляется расстояние до всех остальных кластеров. Наиболее распространённые метрики расстояния:\n",
        "   - Евклидово расстояние\n",
        "   - Манхэттенское расстояние\n",
        "   - Расстояние Махаліноса\n",
        "\n",
        "   Для двух объектов $A(x_1, y_1)$ и $B(x_2, y_2)$ Евклидово расстояние определяется как:\n",
        "\n",
        "$$\n",
        "   d(A, B) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
        "$$\n",
        "\n",
        "3. **Слияние кластеров**: На каждом шаге алгоритм находит два ближайших кластера и объединяет их в один.\n",
        "4. **Повторение**: Шаги 2 и 3 повторяются до тех пор, пока не останется один кластер.\n",
        "\n",
        "#### Пример\n",
        "\n",
        "Рассмотрим простой пример с пятью объектами, заданными координатами в двумерном пространстве:\n",
        "\n",
        "| Объект | Координаты (x, y) |\n",
        "|--------|--------------------|\n",
        "| A      | (1, 2)             |\n",
        "| B      | (2, 3)             |\n",
        "| C      | (6, 5)             |\n",
        "| D      | (7, 8)             |\n",
        "| E      | (3, 4)             |\n",
        "\n",
        "1. **Инициализация**: Каждый объект является отдельным кластером: {A}, {B}, {C}, {D}, {E}.\n",
        "2. **Вычисление расстояний**:\n",
        "\n",
        "   | Кластеры | Расстояние |\n",
        "   |----------|------------|\n",
        "   | {A}, {B} | 1.41       |\n",
        "   | {A}, {C} | 5.0        |\n",
        "   | {A}, {D} | 7.0        |\n",
        "   | {A}, {E} | 2.24       |\n",
        "   | {B}, {C} | 4.47       |\n",
        "   | {B}, {D} | 6.40       |\n",
        "   | {B}, {E} | 1.41       |\n",
        "   | {C}, {D} | 2.83       |\n",
        "   | {C}, {E} | 3.16       |\n",
        "   | {D}, {E} | 5.0        |\n",
        "\n",
        "   Ближайшие кластеры — {A} и {B} (расстояние 1.41).\n",
        "3. **Слияние**: Объединяем {A} и {B} в {A, B}.\n",
        "4. **Повторение**: Рассчитываем расстояния для новых кластеров:\n",
        "\n",
        "   | Кластеры     | Расстояние |\n",
        "   |--------------|------------|\n",
        "   | {A, B}, {C}  | 4.24       |\n",
        "   | {A, B}, {D}  | 6.36       |\n",
        "   | {A, B}, {E}  | 1.41       |\n",
        "   | {C}, {D}     | 2.83       |\n",
        "   | {C}, {E}     | 3.16       |\n",
        "   | {D}, {E}     | 5.0        |\n",
        "\n",
        "   Теперь слияние {A, B} и {E} (расстояние 1.41) станет новым кластером {A, B, E}. И так далее, пока не останется один кластер.\n",
        "\n",
        "### 3. Метод агломерации\n",
        "\n",
        "Существует несколько методов определения расстояния между кластерами:\n",
        "\n",
        "- **Метод ближайшего соседа (Single Linkage)**: расстояние между двумя кластерами определяется как минимальное расстояние между объектами из разных кластеров.\n",
        "  \n",
        "  $$\n",
        "  d(A, B) = \\min_{a \\in A, b \\in B} d(a, b)\n",
        " $$\n",
        "\n",
        "- **Метод дальнего соседа (Complete Linkage)**: расстояние определяется как максимальное расстояние между объектами из разных кластеров.\n",
        "  \n",
        "  $$\n",
        "  d(A, B) = \\max_{a \\in A, b \\in B} d(a, b)\n",
        " $$\n",
        "\n",
        "- **Метод среднего связующего (Average Linkage)**: расстояние — это среднее расстояние между всеми парами объектов из разных кластеров.\n",
        "  \n",
        "  $$\n",
        "  d(A, B) = \\frac{1}{|A| \\cdot |B|} \\sum_{a \\in A} \\sum_{b \\in B} d(a, b)\n",
        " $$\n",
        "\n",
        "- **Метод Вардарса (Ward's Method)**: метод минимизации суммы квадратов внутри кластера. Это расстояние определяется как увеличение общей дисперсии после слияния двух кластеров.\n",
        "\n",
        "### 4. Дендрограмма\n",
        "\n",
        "Дендрограмма — это визуальное представление процесса агломеративной кластеризации. На оси Y отображается расстояние (или мера несходства) между кластерами.\n",
        "\n",
        "### 5. Алгоритм делительной кластеризации\n",
        "\n",
        "Делительная кластеризация менее распространена, но также важна. В отличие от агломеративного метода, он начинается с одного кластера и последовательно делит его на подмножества.\n",
        "\n",
        "1. **Инициализация**: Все объекты находятся в одном кластере.\n",
        "2. **Разделение кластера**: Выбирается кластер для разделения. Внутри этого кластера используется один из методов кластеризации для разделения его на более мелкие кластеры (например, K-средние).\n",
        "3. **Повторение**: Шаги 2 и 3 повторяются, пока не достигнута желаемая структура кластеров.\n",
        "\n",
        "### 6. Пример применения иерархической кластеризации\n",
        "\n",
        "Рассмотрим использование иерархической кластеризации для анализа набора данных о цветах, в котором представлены различные виды цветов и их характеристики, такие как длина и ширина лепестков.\n",
        "\n",
        "1. **Данные**:\n",
        "\n",
        "   | Вид         | Длина лепестка | Ширина лепестка |\n",
        "   |-------------|----------------|------------------|\n",
        "   | Iris-setosa | 1.4            | 0.2              |\n",
        "   | Iris-versicolor | 4.7         | 1.4              |\n",
        "   | Iris-virginica | 6.0         | 2.5              |\n",
        "   | ...         | ...            | ...              |\n",
        "\n",
        "2. **Стандартные метрики расстояния**: используется Евклидово расстояние.\n",
        "\n",
        "3. **Агломеративный алгоритм**: определяется количество кластеров, которые нужно сформировать, с помощью дендрограммы.\n",
        "\n",
        "4. **Результаты**: на основе дендрограммы исследователь может выбрать уровень слияния, который отражает желаемое количество кластеров.\n",
        "\n",
        "### 7. Преимущества и недостатки\n",
        "\n",
        "#### Преимущества\n",
        "\n",
        "- Простота и наглядность.\n",
        "- Не требует предварительного задания количества кластеров.\n",
        "- Подходит для анализа многомерных данных.\n",
        "\n",
        "#### Недостатки\n",
        "\n",
        "- Вычислительная сложность, особенно для больших наборов данных ($O(n^3)$).\n",
        "- Чувствительность к шуму и выбросам.\n",
        "- Результаты могут зависеть от выбранной метрики расстояния и метода агломерации.\n",
        "\n",
        "Хорошо! Давайте рассмотрим подробные математические примеры применения анализа главных компонент (PCA). Мы разобьем процесс на этапы и предоставим детальные математические решения.\n",
        "\n",
        "## Пример 1: Анализ данных о росте и весе\n",
        "\n",
        "### Данные\n",
        "\n",
        "Рассмотрим набор данных о росте и весе для 5 человек:\n",
        "\n",
        "| Человек | Рост (см) | Вес (кг) |\n",
        "|---------|-----------|----------|\n",
        "| 1       | 170       | 70       |\n",
        "| 2       | 160       | 60       |\n",
        "| 3       | 180       | 80       |\n",
        "| 4       | 165       | 65       |\n",
        "| 5       | 175       | 75       |\n",
        "\n",
        "### Шаг 1: Стандартизация данных\n",
        "\n",
        "Сначала найдем средние значения и стандартные отклонения для каждого признака.\n",
        "\n",
        "#### Средние значения\n",
        "\n",
        "$$\n",
        "\\mu_{\\text{рост}} = \\frac{170 + 160 + 180 + 165 + 175}{5} = \\frac{850}{5} = 170\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mu_{\\text{вес}} = \\frac{70 + 60 + 80 + 65 + 75}{5} = \\frac{350}{5} = 70\n",
        "$$\n",
        "\n",
        "#### Стандартные отклонения\n",
        "\n",
        "Стандартное отклонение для роста:\n",
        "\n",
        "$$\n",
        "\\sigma_{\\text{рост}} = \\sqrt{\\frac{(170 - 170)^2 + (160 - 170)^2 + (180 - 170)^2 + (165 - 170)^2 + (175 - 170)^2}{5 - 1}} = \\sqrt{\\frac{0 + 100 + 100 + 25 + 25}{4}} = \\sqrt{\\frac{250}{4}} = \\sqrt{62.5} \\approx 7.91\n",
        "$$\n",
        "\n",
        "Стандартное отклонение для веса:\n",
        "\n",
        "$$\n",
        "\\sigma_{\\text{вес}} = \\sqrt{\\frac{(70 - 70)^2 + (60 - 70)^2 + (80 - 70)^2 + (65 - 70)^2 + (75 - 70)^2}{5 - 1}} = \\sqrt{\\frac{0 + 100 + 100 + 25 + 25}{4}} = \\sqrt{\\frac{250}{4}} = \\sqrt{62.5} \\approx 7.91\n",
        "$$\n",
        "\n",
        "#### Стандартизированные данные\n",
        "\n",
        "Теперь можем стандартизировать данные:\n",
        "\n",
        "$$\n",
        "Z_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j}\n",
        "$$\n",
        "\n",
        "| Человек | Рост (Z)               | Вес (Z)                |\n",
        "|---------|-----------------------|-----------------------|\n",
        "| 1       | $Z_{1,1} = \\frac{170 - 170}{7.91} = 0$  | $Z_{1,2} = \\frac{70 - 70}{7.91} = 0$    |\n",
        "| 2       | $Z_{2,1} = \\frac{160 - 170}{7.91} \\approx -1.27$ | $Z_{2,2} = \\frac{60 - 70}{7.91} \\approx -1.27$ |\n",
        "| 3       | $Z_{3,1} = \\frac{180 - 170}{7.91} \\approx 1.27$  | $Z_{3,2} = \\frac{80 - 70}{7.91} \\approx 1.27$  |\n",
        "| 4       | $Z_{4,1} = \\frac{165 - 170}{7.91} \\approx -0.63$ | $Z_{4,2} = \\frac{65 - 70}{7.91} \\approx -0.63$ |\n",
        "| 5       | $Z_{5,1} = \\frac{175 - 170}{7.91} \\approx 0.63$  | $Z_{5,2} = \\frac{75 - 70}{7.91} \\approx 0.63$  |\n",
        "\n",
        "Теперь у нас есть матрица стандартизированных данных:\n",
        "\n",
        "$$\n",
        "Z = \\begin{pmatrix}\n",
        "0     & 0     \\\\\n",
        "-1.27 & -1.27 \\\\\n",
        "1.27  & 1.27  \\\\\n",
        "-0.63 & -0.63 \\\\\n",
        "0.63  & 0.63\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 2: Вычисление ковариационной матрицы\n",
        "\n",
        "Ковариационная матрица $C$ вычисляется как:\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "где $n$ — количество наблюдений.\n",
        "\n",
        "Подсчитаем:\n",
        "\n",
        "$$\n",
        "Z^T = \\begin{pmatrix}\n",
        "0     & -1.27 & 1.27  & -0.63 & 0.63  \\\\\n",
        "0     & -1.27 & 1.27  & -0.63 & 0.63\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим произведение $Z^T Z$:\n",
        "\n",
        "$$\n",
        "Z^T Z = \\begin{pmatrix}\n",
        "0^2 + (-1.27)^2 + (1.27)^2 + (-0.63)^2 + (0.63)^2 & 0 \\cdot 0 + (-1.27) \\cdot (-1.27) + 1.27 \\cdot 1.27 + (-0.63) \\cdot (-0.63) + 0.63 \\cdot 0.63 \\\\\n",
        "0 \\cdot 0 + (-1.27) \\cdot (-1.27) + 1.27 \\cdot 1.27 + (-0.63) \\cdot (-0.63) + 0.63 \\cdot 0.63 & 0^2 + (-1.27)^2 + (1.27)^2 + (-0.63)^2 + (0.63)^2\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, получим:\n",
        "\n",
        "$$\n",
        "Z^T Z = \\begin{pmatrix}\n",
        "0 + 1.6129 + 1.6129 + 0.3969 + 0.3969 & 0 \\\\\n",
        "0 & 0 + 1.6129 + 1.6129 + 0.3969 + 0.3969\n",
        "\\end{pmatrix}\n",
        "= \\begin{pmatrix}\n",
        "4.0196 & 0 \\\\\n",
        "0 & 4.0196\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим ковариационную матрицу:\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{5-1} Z^T Z = \\frac{1}{4} \\begin{pmatrix}\n",
        "4.0196 & 0 \\\\\n",
        "0 & 4.0196\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "1.0049 & 0 \\\\\n",
        "0 & 1.0049\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 3: Нахождение собственных значений и собственных векторов\n",
        "\n",
        "Теперь найдем собственные значения и собственные векторы матрицы ковариации $C$.\n",
        "\n",
        "Решаем характеристическое уравнение:\n",
        "\n",
        "$$\n",
        "\\text{det}(C - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "где $I$ — единичная матрица. То есть:\n",
        "\n",
        "$$\n",
        "\\text{det}\\left(\\begin{pmatrix}\n",
        "1.0049 - \\lambda & 0 \\\\\n",
        "0 & 1.0049 - \\lambda\n",
        "\\end{pmatrix}\\right) = (1.0049 - \\lambda)^2 = 0\n",
        "$$\n",
        "\n",
        "Таким образом, находим собственные значения:\n",
        "\n",
        "$$\n",
        "\\lambda_1 = \\lambda_2 = 1.0049\n",
        "$$\n",
        "\n",
        "Теперь найдем собственные векторы. Подставим $\\lambda$ в уравнение:\n",
        "\n",
        "$$\n",
        "(C - \\lambda I)v = 0\n",
        "$$\n",
        "\n",
        "Получаем:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1.0049 - \\lambda & 0 \\\\\n",
        "0 & 1.0049 - \\lambda\n",
        "\\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = 0\n",
        "$$\n",
        "\n",
        "Поскольку оба собственных значения равны, это говорит о том, что у нас два одинаковых собственных вектора:\n",
        "\n",
        "$$\n",
        "v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\quad \\text{и} \\quad v = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 4: Сортировка собственных векторов\n",
        "\n",
        "В данном случае оба собственных вектора имеют одинаковую важность, и мы можем выбрать любой из них для проекции данных.\n",
        "\n",
        "### Шаг 5: Проекция данных\n",
        "\n",
        "Проектируем стандартизированные данные на первую главную компоненту:\n",
        "\n",
        "$$\n",
        "Y = Z v\n",
        "$$\n",
        "\n",
        "Выберем, например, собственный вектор $v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$:\n",
        "\n",
        "$$\n",
        "Y = Z \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} =\n",
        "\n",
        " \\begin{pmatrix}\n",
        "0 \\\\\n",
        "-1.27 \\\\\n",
        "1.27 \\\\\n",
        "-0.63 \\\\\n",
        "0.63\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Результаты\n",
        "\n",
        "Мы получили проекцию исходных данных на главную компоненту. Эта информация может быть использована для дальнейшего анализа.\n",
        "\n",
        "## Пример 2: Анализ изображений\n",
        "\n",
        "Рассмотрим простой пример с изображениями, где каждый пиксель будет представлять собой переменную.\n",
        "\n",
        "### Данные\n",
        "\n",
        "Предположим, у нас есть 4 изображения (по 4 пикселя каждое), которые представлены следующим образом:\n",
        "\n",
        "| Изображение | Пиксель 1 | Пиксель 2 | Пиксель 3 | Пиксель 4 |\n",
        "|-------------|-----------|-----------|-----------|-----------|\n",
        "| 1           | 1         | 2         | 3         | 4         |\n",
        "| 2           | 2         | 3         | 4         | 5         |\n",
        "| 3           | 3         | 4         | 5         | 6         |\n",
        "| 4           | 4         | 5         | 6         | 7         |\n",
        "\n",
        "### Шаг 1: Стандартизация данных\n",
        "\n",
        "Найдем средние значения и стандартные отклонения для каждого пикселя.\n",
        "\n",
        "#### Средние значения\n",
        "\n",
        "$$\n",
        "\\mu_{1} = \\frac{1 + 2 + 3 + 4}{4} = \\frac{10}{4} = 2.5\n",
        "$$\n",
        "$$\n",
        "\\mu_{2} = \\frac{2 + 3 + 4 + 5}{4} = \\frac{14}{4} = 3.5\n",
        "$$\n",
        "$$\n",
        "\\mu_{3} = \\frac{3 + 4 + 5 + 6}{4} = \\frac{18}{4} = 4.5\n",
        "$$\n",
        "$$\n",
        "\\mu_{4} = \\frac{4 + 5 + 6 + 7}{4} = \\frac{22}{4} = 5.5\n",
        "$$\n",
        "\n",
        "#### Стандартные отклонения\n",
        "\n",
        "$$\n",
        "\\sigma_{1} = \\sqrt{\\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4 - 1}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{3}} = \\sqrt{\\frac{5}{3}} \\approx 1.29\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_{2} = \\sqrt{\\frac{(2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2}{3}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{3}} = \\sqrt{\\frac{5}{3}} \\approx 1.29\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_{3} = \\sqrt{\\frac{(3-4.5)^2 + (4-4.5)^2 + (5-4.5)^2 + (6-4.5)^2}{3}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{3}} = \\sqrt{\\frac{5}{3}} \\approx 1.29\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_{4} = \\sqrt{\\frac{(4-5.5)^2 + (5-5.5)^2 + (6-5.5)^2 + (7-5.5)^2}{3}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{3}} = \\sqrt{\\frac{5}{3}} \\approx 1.29\n",
        "$$\n",
        "\n",
        "#### Стандартизированные данные\n",
        "\n",
        "Теперь стандартизируем данные:\n",
        "\n",
        "$$\n",
        "Z_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j}\n",
        "$$\n",
        "\n",
        "| Изображение | Пиксель 1 (Z) | Пиксель 2 (Z) | Пиксель 3 (Z) | Пиксель 4 (Z) |\n",
        "|-------------|----------------|----------------|----------------|----------------|\n",
        "| 1           | $\\frac{1 - 2.5}{1.29} \\approx -1.16$ | $\\frac{2 - 3.5}{1.29} \\approx -1.16$ | $\\frac{3 - 4.5}{1.29} \\approx -1.16$ | $\\frac{4 - 5.5}{1.29} \\approx -1.16$ |\n",
        "| 2           | $\\frac{2 - 2.5}{1.29} \\approx -0.39$ | $\\frac{3 - 3.5}{1.29} \\approx -0.39$ | $\\frac{4 - 4.5}{1.29} \\approx -0.39$ | $\\frac{5 - 5.5}{1.29} \\approx -0.39$ |\n",
        "| 3           | $\\frac{3 - 2.5}{1.29} \\approx 0.39$ | $\\frac{4 - 3.5}{1.29} \\approx 0.39$ | $\\frac{5 - 4.5}{1.29} \\approx 0.39$ | $\\frac{6 - 5.5}{1.29} \\approx 0.39$ |\n",
        "| 4           | $\\frac{4 - 2.5}{1.29} \\approx 1.16$ | $\\frac{5 - 3.5}{1.29} \\approx 1.16$ | $\\frac{6 - 4.5}{1.29} \\approx 1.16$ | $\\frac{7 - 5.5}{1.29} \\approx 1.16$ |\n",
        "\n",
        "Стандартизированная матрица:\n",
        "\n",
        "$$\n",
        "Z = \\begin{pmatrix}\n",
        "-1.16 & -1.16 & -1.16 & -1.16 \\\\\n",
        "-0.39 & -0.39 & -0.39 & -0.39 \\\\\n",
        "0.39  & 0.39  & 0.39  & 0.39  \\\\\n",
        "1.16  & 1.16  & 1.16  & 1.16\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 2: Вычисление ковариационной матрицы\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "Вычисляем $Z^T$ и $Z^T Z$:\n",
        "\n",
        "$$\n",
        "Z^T = \\begin{pmatrix}\n",
        "-1.16 & -0.39 & 0.39 & 1.16 \\\\\n",
        "-1.16 & -0.39 & 0.39 & 1.16 \\\\\n",
        "-1.16 & -0.39 & 0.39 & 1.16 \\\\\n",
        "-1.16 & -0.39 & 0.39 & 1.16\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Теперь найдем:\n",
        "\n",
        "$$\n",
        "Z^T Z = \\begin{pmatrix}\n",
        "(-1.16)^2 + (-0.39)^2 + (0.39)^2 + (1.16)^2 & ... \\\\\n",
        "... & ...\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Вычисляя каждую ячейку, мы получаем:\n",
        "\n",
        "$$\n",
        "Z^T Z = \\begin{pmatrix}\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычисляем ковариационную матрицу:\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{4 - 1} Z^T Z = \\frac{1}{3} \\begin{pmatrix}\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "1.33 & 1.33 & 1.33 & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 & 1.33\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 3: Нахождение собственных значений и собственных векторов\n",
        "\n",
        "Решаем характеристическое уравнение:\n",
        "\n",
        "$$\n",
        "\\text{det}(C - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "Т.е.\n",
        "\n",
        "$$\n",
        "\\text{det}\\left(\\begin{pmatrix}\n",
        "1.33 - \\lambda & 1.33 & 1.33 & 1.33 \\\\\n",
        "1.33\n",
        "\n",
        " & 1.33 - \\lambda & 1.33 & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 - \\lambda & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 & 1.33 - \\lambda\n",
        "\\end{pmatrix}\\right) = 0\n",
        "$$\n",
        "\n",
        "Решая это уравнение, находим собственные значения и собственные векторы.\n",
        "\n",
        "### Шаг 4: Проекция данных\n",
        "\n",
        "Для проекции используем найденные собственные векторы и получим уменьшенное представление данных.\n",
        "\n",
        "### Заключение\n",
        "\n",
        "PCA — мощный метод анализа данных, который помогает уменьшить размерность и выявить основные компоненты в наборе данных. Мы рассмотрели два примера с детальными математическими шагами, которые иллюстрируют, как проводить анализ главных компонент на практических данных.\n",
        "\n",
        " Давайте реализуем анализ главных компонент (PCA) на Python. Мы воспользуемся библиотеками `numpy` и `pandas` для работы с данными и `matplotlib` для визуализации.\n",
        "\n",
        "### Пример 1: Анализ данных о росте и весе\n",
        "\n",
        "#### Установка необходимых библиотек\n",
        "\n",
        "Если у вас еще не установлены библиотеки, вы можете установить их с помощью следующей команды:\n",
        "\n",
        "```bash\n",
        "pip install numpy pandas matplotlib scikit-learn\n",
        "```\n",
        "\n",
        "#### Код\n",
        "\n",
        "Теперь давайте напишем код для выполнения PCA на примере данных о росте и весе.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Данные о росте и весе\n",
        "data = {\n",
        "    'Рост': [170, 160, 180, 165, 175],\n",
        "    'Вес': [70, 60, 80, 65, 75]\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Стандартизация данных\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Применяем PCA\n",
        "pca = PCA(n_components=1)\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Создаем DataFrame с главными компонентами\n",
        "principal_df = pd.DataFrame(data=principal_components, columns=['Первая главная компонента'])\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(principal_df['Первая главная компонента'], np.zeros_like(principal_df['Первая главная компонента']),\n",
        "            color='blue', alpha=0.6)\n",
        "plt.title('PCA: Первая главная компонента')\n",
        "plt.xlabel('Первая главная компонента')\n",
        "plt.yticks([])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Пример 2: Анализ изображений\n",
        "\n",
        "Теперь давайте реализуем PCA для простого примера с изображениями (по 4 пикселя каждое).\n",
        "\n",
        "#### Код\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Данные об изображениях (4 пикселя на изображение)\n",
        "data = {\n",
        "    'Пиксель_1': [1, 2, 3, 4],\n",
        "    'Пиксель_2': [2, 3, 4, 5],\n",
        "    'Пиксель_3': [3, 4, 5, 6],\n",
        "    'Пиксель_4': [4, 5, 6, 7]\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Стандартизация данных\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Применяем PCA\n",
        "pca = PCA(n_components=2)  # Уменьшаем до 2-х компонентов для визуализации\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Создаем DataFrame с главными компонентами\n",
        "principal_df = pd.DataFrame(data=principal_components, columns=['Первая главная компонента', 'Вторая главная компонента'])\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(principal_df['Первая главная компонента'], principal_df['Вторая главная компонента'], color='red', alpha=0.6)\n",
        "plt.title('PCA: Главные компоненты изображений')\n",
        "plt.xlabel('Первая главная компонента')\n",
        "plt.ylabel('Вторая главная компонента')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Запуск кода\n",
        "\n",
        "1. Скопируйте каждый блок кода в отдельные ячейки вашего Python-окружения или в один файл Python.\n",
        "2. Запустите код, и вы должны увидеть визуализации PCA для обоих примеров.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QWicxD_oPYav"
      }
    }
  ]
}